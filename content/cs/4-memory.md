---
title: 메모리
aliases:
  - memory-cache-memory
description: 저장을 위한 기억 장치
draft: false
tags:
  - CS/computer
permalink: /memory
created: 2025-04-22T00:07
updated: 2025-04-24T13:57
socialImage: https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExanNrbGRjZm91OHdzeG0xbnEzcmFxaTYzaXAyZzR1N3IweHNyb3RxYSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/BkfAhfmX0Ppn2/giphy.gif
---
<p align="center">
  <img src="https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExanNrbGRjZm91OHdzeG0xbnEzcmFxaTYzaXAyZzR1N3IweHNyb3RxYSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/BkfAhfmX0Ppn2/giphy.gif" alt="memory" width="300">
</p>

#  메모리

핵심 부품 중 주 기억 장치는 **메모리**라고 통칭한다. 메인 메모리에는 RAM과 ROM으로 구성되어 있지만, 일반적으로 메모리는 RAM을 지칭한다. 모든 명령어를 처리하기 위해서는 CPU가 읽고, 해석하고, 실행하는 것 들은 어디에 저장이 되어 있어야 만 한다. 즉, **실행 중인 프로그램을 구성하는 데이터와 명령어를 저장**하는 곳이 바로 **메모리**다.

다만, 메모리의 경우 **주소**의 위치에 **휘발성**이 강하게 저장하는 특징을 보인다. 저장된 내용은 각각의 **주소**를 가지고 있어, CPU는 해당 주소를 통해 명령어와 데이터를 찾아낸다. 컴퓨터는 수 많은 데이터와 명령어를 활용하여 프로그램을 실행하는데, 이를 RAM에다가 계속해서 저장할 수는 없다. 따라서 전원이 공급되고 있지 않을 때는 삭제하는 **휘발성**의 특징을 지닌다. 컴퓨터가 느려지면 재부팅 하는 것이 이 RAM을 초기화 한다고 보면 된다.

CPU와 메모리 사이에는 **캐시 메모리**가 있다. 캐시 메모리는 CPU가 조금 더 빨리 메모리에 접근 할 수 있도록 하는 저장장치다. 이는 하단에서 알아보자

## 우리가 자주 쓰는 RAM

CPU는 보조기억장치에 저장된 프로그램을 바로 실행할 수 없기에 메모리에 복사 후 사용하게 된다. 따라서 RAM의 용량은 컴퓨터 성능에 큰 영향을 미친다는 것이다. RAM이 작으면 보조기억장치에서 프로그램을 복사하는 시간이 잦아 실행 시간이 길어지지만, RAM의 용량이 크면 많은 데이터를 복사할 수 있기 때문이다.

RAM은 Random Access Memory, 임의 접근 메모리의 약자이다. **임의 접근**은 순차적으로 접근하지 않고 임의의 위치에 곧장 접근하는 방식을 의미한다. 그래서 **직접 접근**이라고 불리기도 한다. 주소가 있다면 바로 접근할 수 있기 때문에 어떤 주소에 접근하든 데이터에 접근하는 시간이 동일하다는 특징을 갖는다.

이와는 반대로 **순차 접근**이라는 것이 있다. 데이터에 접근하기 위해선 순차적으로 접근한다는 것인데, 데이터의 주소에 따라 접근하는 시간이 달라질 수 있다. Array 자료구조와 마찬가지로 각 순차적으로 접근하는 것이 이와 동일한 예제이다.

RAM을 구매할 때 보면 RAM도 여러 종류가 많다는 것을 알 수 있다. DRAM, SRAM, SDRAM, DDR SDRAM 등이 있다. RAM이 컴퓨터 성능에 도움을 준다면, 종류와 역할에 맞게 해야 한다.

1. **DRAM**  
DRAM은 Dynamic RAM으로서 동적인 특징을 갖는 RAM이다. 즉, 시간이 지나면 저장된 데이터가 점점 사라진다는 것이다. 이를 위해서 일정 주기로 데이터를 재활성화(재 저장)해야 한다.  DRAM은 소비 전력도 낮고, 저렴하며, 집적도가 높아 메모리를 대용량으로 설계할 때 많이 사용하기 때문에 일반적으로 제일 많이 사용한다.
</br></br>
2. **SRAM**  
DRAM이 나왔으니 SRAM, Static RAM으로서 정적인 특징을 갖는 RAM이다. 즉, 시간이 지나도 저장된 데이터가 사라지지 않는다. 일반적으로 SRAM은 DRAM보다 속도는 빠르고 소비 전력도 많이 잡아먹는다. 즉, 비싸고 집적도도 낮다. 그렇기에 대용량으로 설계하지 않지만 빠른 속도가 필요한 저장장치, **캐시 메모리**에 사용하는 것이 일반적이다.
</br></br>
3. **SDRAM**  
*(DRAM+SRAM이 아니고)* Synchronous Dynamic RAM, 클럭 신호와 동기화 된, 보다 발전된 DRAM을 의미한다. 클럭 신호와 동기화 되었기 때문에 클럭 타이밍과 맞춰 CPU와 정보 전달을 하는 특징을 갖는다.
</br></br>
4. **DDR SDRAM**  
DDR SDRAM, Double Data Rate SDRAM. 대역폭을 넓혀 속도를 빠르게 만든 SDRAM이다. 여기서 말하는 **대역폭**이란, 데이터를 주고 받을 길의 너비를 의미한다.  SDRAM이 클럭 한 번 당 한 번 씩 CPU와 통신한다면, DDR SRAM은 두 번 씩 CPU와 통신한다는 것을 의미한다. 즉, SDRAM보다 전송 속도가 두 배 가량 빠르다.
^lane

여기서 우리가 많이 들어본 내용이 나온다. **DDR2 SDRAM**은 DDR SDRAM보다 4배 빠르고 ($2^2$배), DDR3 SDRAM은 ($2^3$배), DDR4 SDRAM은 ($2^4$배)이다. 이게 [지금 우리 컴퓨터에 꽂혀있는 RAM](https://www.google.com/search?q=ddr4&sourceid=chrome&ie=UTF-8)이다.

### 메모리에 데이터 저장 방법

메모리는 대부분 Byte단위로 저장하고 관리한다. 하지만 메모리는 데이터를 CPU로 부터 Byte가 아닌 4byte ($2^5$bit) 혹은 8byte($2^6$bit)인 [[2-readable-data#^word-def|워드]] 단위로 받아 들인다. 그렇게 받아들인 데이터는 여러 주소에 걸쳐 저장한다. 즉, 한 주소에 1byte씩 저장하는 메모리는 4byte의 데이터를 4개의 주소에 저장한다는 것이다. 이 때 저장하는 방식이 다른데, 이는 두 가지 방식이 있다. **빅 엔디안**과 **리틀 엔디안**이다. 

**빅 엔디안**은 **낮은 번지의 주소에 상위 바이트부터 저장하는 방식**을 의미한다. 가장 큰 수를 가장 낮은 번지에 저장하고 순차적으로 높은 번지에 저장하는 방식이다. **리틀 엔디안**은 그 반대로, **낮은 번지의 주소에 하위 바이트부터 저장**하는 방식이다. 빅 엔디안은 MSB가 있는 바이트, 즉 중요하고 큰 데이터부터 저장해 나가는 방식이고, 리틀 엔디안은 LSB로 덜 중요하고 작은 것 부터 저장해 나가는 방식이다.

> [!note] **MSB** & **LSB**
> MSB: Most Significant Bit, 숫자의 크기에 가장 큰 영향을 미치는 유효 숫자.   
> LSB: Least Significant Bit, 숫자의 크기에 가장 적은 영향을 미치는 숫자. 

빅 엔디안은 MSB를 중요하다고 판단하여 낮은 메모리 주소에 저장한다. 이는 숫자 체계와 동일하기 때문에 메모리 값을 직접 읽거나 디버깅에 유리하다. 이와 반대로 리틀 엔디안은 LSB를 가장 낮은 메모리 주소에 저장하는 방식이기 때문에 직접 읽고 쓰는 것에는 비효율적이지만 수를 계산하는 것에 유리하다.  (둘 중 하나를 선택하는 것은 **바이 엔디안** 이라고 함)

## 캐시 메모리, CPU와 메모리의 다리
^cache-def

CPU는 프로그램을 사용하기 위해서 메모리에 빈번하게 접근해야 한다. 하지만 CPU가 메모리에 접근하는 속도는 레지스터에 접근하는 속도보다 느리기에 메모리에 접근하는 속도가 느리면 CPU가 아무리 좋더라도 소용이 없다. 그렇다면 속도를 어떻게 더 빠르게 할 수 있을까?

이를 해결하기 위해 **캐시 메모리**가 등장한다. CPU의 연산 속도와 메모리 접근 속도의 차이를 줄이는 것에는 캐시 메모리가 최고다. 아까 RAM의 종류에서 언급한 SRAM 기반의 저장장치이다. CPU가 메모리에 직접 접근하게 하지 않고, 메모리에서 CPU가 사용할 일부 데이터를 RAM에서 미리 캐시 메모리에 복사해두고 활용하는 방식을 의미한다.

컴퓨터에는 여러 캐시 메모리가 존재하는데, 코어를 기준으로 **L1, L2, L3**가 있다. 일반적으로는 코어 내에 L1, L2가 있고 L3은 캐시 메모리 외부에 위치해 있다. 당연하게도 크기는 L1 < L2 < L3 이지만 속도는 L3 < L2 < L1 순으로 빠르다. 코어 내에 있기 때문에 작지만 가까우니 빠르다는 것이다. 여기서 가장 작고 빠른 L1은 처리 효율성을 높이기 위해 명령어 저장만 하는 **L1I**와 데이터만 저장하는 **L1D**로 나뉜다. 이러한 캐시 메모리가 있다면 **분리형 캐시**라고 한다.

이는 곧 CPU가 캐시 메모리를 활용하는 방식에서 보여진다. CPU는 데이터가 필요하게 되면 우선적으로 **L1**에서 데이터가 있는지 찾는다. 없으면 L2, 그리고 L3에서 검색한다. 하지만, 캐시 메모리의 용량은 일반적인 메모리 용량보다 훨씬 적다. 따라서 모든 내용을 캐시 메모리에 저장할 수 없다. 

캐시 메모리도 메모리의 일부를 저장한다. 그 중 **보조 저장 장치가 보관할 것** 과 메모리가 **실행 중인 것**을 저장하여 CPU가 사용할 법한 것을 저장한다. 맞다. CPU가 쓸 것 들을 **예측**해서 저장한다는 의미이다. 

예측해서 저장한 것들이 CPU가 실제로 사용되는 경우에는 예측에 성공했다는 의미로, **캐시 히트** 라고 하며 실패해서 CPU가 메모리에 직접 가져와야 하는 상황이라면 **캐시 미스**라고 한다. AI를 해봐서 알겠지만, 이는 확률에 기반한다. 정확도가 높으면 높을 수록 성능이 좋은 모델처럼, 캐시 메모리도 예측을 잘 한다면, 그 만큼 성능이 좋다고 볼 수 있다. 따라서 캐시 메모리가 예측에 성공할 확률은 **캐시 적중률** 이라고 한다. 이게 낮다면 캐시 메모리의 이점을 활용할 수 없게 되고 CPU의 성능은 계속 하락하는 것이다.

### 이점은 참조 지역성의 원리로

그렇다면 캐시 적중률을 어떻게 향상시킬 수 있을까. 이는 **참조 지역성의 원리**라는 원칙에 따랐을 때 이점을 최대한 활용할 수 있다. 참조 지역성의 원리는 시간 지역성과 공간 지역성으로 나누어서 볼 수 있다.

| 참조 지역성의 원리 |                                    |
| ---------- | ---------------------------------- |
| 시간 지역성     | CPU는 최근에 접근했던 메모리 공간에 접근하는 경향이 있다  |
| 공간 지역성     | CPU는 접근했던 공간 근처의 공간에 접근하려는 경향이 있다. |

마치 사람과 비슷하다. 최근에 했던 것을 반복하는 습관과 같은 것이 시간 지역성이다. 이는 프로그래밍 언어에서 자주 보이는데, **변수** 설정이 이와 비슷하다. 우리는 프로그래밍을 할 때 변수를 저장해두고 사용한다. 이 변수는 한 번만 사용하는 것이 아니라 여러 번 사용하기 때문에 시간 지역성이라고 볼 수 있다.

이와는 달리 공간 지역성은 **Array, 배열**이 그렇다. 메모리는 스택의 자료 구조에 따라 저장된다고 배웠다. 그렇다면 2차원 배열에서 데이터를 추출할 때, 순차적으로, 행 단위로 추출하는 것이 빠르지 열 단위로 추출한다면 상대적으로 느려진다. 이것이 바로 공간 지역성이다. 공간 지역성을 고려한다면 행 단위로 추출하는 것이고 고려하지 않는다면 열 단위로 추출하는 것이다.

### 캐시 메모리에서 데이터 추출 방법

CPU는 캐시 메모리에서 데이터를 쓸 때 캐시 메모리에 새롭게 쓰여진 데이터와 메모리 상의 데이터가 일관성을 유지해야 한다. 가령 CPU가 메모리내 데이터를 변경하려고 한다고 가정했을 때, 메모리의 데이터를 직접 변경한다면 캐시 메모리와 일치하지 않기 때문에 결과가 다를 가능성이 높다. 이를 방지하기 위해 메모리는 2가지 방식을 가지고 있다. 

첫 번째는 **즉시 쓰기**이다. 캐시 메모리와 메모리에 동시에 적용하는 것이다. 일관성을 유지하는 것이 가장 중요하기 때문에 메모리를 항상 최신 상태로 유지한다. 하지만 이는 데이터를 쓸 때 마다 메모리를 참고해야 하기 때문에 버스 사용 시간과 쓰기 시간이 늘어날 수 밖에 없다. 메모리 접근을 최소화하기 위해 캐시 메모리를 사용하고 있는데, 비효율적으로 사용하는 것과 마찬가지이기 때문이다.

두 번째는 **지연 쓰기**이다. 캐시 메모리에만 써두고 이후 수정된 데이터를 메모리에 반영하는 것이다. 메모리에 접근 횟수를 줄이기 때문에 속도는 빠르지만, 메모리와 캐시 메모리 사이 일관성이 깨질 수 있다는 단점을 감수해야 한다. 지연 쓰기는 지금 이 상황에서도 많이 발생한다. 특히 현재 이 Blog를 작성할 때도 네트워크 캐시 메모리와 현재 로컬 메모리와의 차이로 인해서 반영이 느리거나 되지 않는 경우가 많다 (일관성 깨짐). 

캐시 메모리를 사용하면서 생기는 문제는 앞서 말했드시 로컬 뿐 아니라 네트워크 상황에서도 이루어지는 것이다. 하지만 **"자주 사용할 법한 대상을 가깝게 위치해서, 성능 향상을 이끌어낸다"** 의 이론은 동일하다는 것.

</br></br></br>
# 참고자료

※ 이 글은 [『이것이 컴퓨터 과학이다』](https://product.kyobobook.co.kr/detail/S000214014967) 책을 기반으로, 다양한 자료를 참고해 작성했습니다.